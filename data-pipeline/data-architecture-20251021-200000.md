# Data Pipeline Architecture - Decentralized Autonomous Forum
**Generated By**: Data Agent
**Date**: 2025-10-21 20:00:00
**Project**: project-20251021-092500-decentralized-forum
**Version**: 1.0
**Status**: Ready for Development

---

## Executive Summary

This document defines the complete data pipeline architecture, analytics framework, and data governance strategy for the Decentralized Autonomous Forum platform. The data infrastructure follows modern DataOps principles and enables:

- **Real-Time Analytics**: User engagement, point economy, content performance
- **Data-Driven Insights**: User behavior patterns, growth metrics, moderation effectiveness
- **GDPR Compliance**: Privacy-by-design, data minimization, user data rights
- **Scalable Data Infrastructure**: PostgreSQL + Redis + analytical processing

**Key Components**: 5 data pipelines, 8 analytics dashboards, 12 data quality monitors
**Data Volume**: 10K MAU target = ~500K transactions/month, ~100K posts/month
**Processing**: Real-time (event stream) + Batch (daily aggregations)

---

## Table of Contents

1. [Data Architecture Overview](#1-data-architecture-overview)
2. [Data Pipeline Design](#2-data-pipeline-design)
3. [Analytics & Reporting](#3-analytics--reporting)
4. [Data Governance & Quality](#4-data-governance--quality)
5. [Implementation Strategy](#5-implementation-strategy)

---

## 1. Data Architecture Overview

### 1.1 Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Architecture                            â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚
â”‚  â”‚ Application â”‚ (FastAPI)                                      â”‚
â”‚  â”‚   Events    â”‚                                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
â”‚         â”‚                                                        â”‚
â”‚         â”‚ Writes                                                 â”‚
â”‚         â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚          PostgreSQL 16+ (OLTP)                      â”‚       â”‚
â”‚  â”‚                                                       â”‚       â”‚
â”‚  â”‚  â€¢ users, posts, comments, transactions             â”‚       â”‚
â”‚  â”‚  â€¢ Real-time reads/writes                           â”‚       â”‚
â”‚  â”‚  â€¢ ACID transactions                                â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â”‚                         â”‚                            â”‚
â”‚         â”‚ CDC/Triggers            â”‚ Materialized Views        â”‚
â”‚         â–¼                         â–¼                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  Redis Cache â”‚        â”‚  Analytics Views â”‚                 â”‚
â”‚  â”‚              â”‚        â”‚                  â”‚                 â”‚
â”‚  â”‚ â€¢ Real-time  â”‚        â”‚ â€¢ v_trending_postsâ”‚                â”‚
â”‚  â”‚   metrics    â”‚        â”‚ â€¢ v_leaderboard  â”‚                â”‚
â”‚  â”‚ â€¢ Counters   â”‚        â”‚ â€¢ v_active_users â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚         â”‚                         â”‚                            â”‚
â”‚         â”‚                         â”‚ Queries                    â”‚
â”‚         â–¼                         â–¼                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚       Analytics & Dashboards              â”‚                 â”‚
â”‚  â”‚                                            â”‚                 â”‚
â”‚  â”‚  â€¢ User Engagement Metrics                â”‚                 â”‚
â”‚  â”‚  â€¢ Point Economy Analytics                â”‚                 â”‚
â”‚  â”‚  â€¢ Content Performance Reports            â”‚                 â”‚
â”‚  â”‚  â€¢ Moderation Dashboard                   â”‚                 â”‚
â”‚  â”‚  â€¢ Growth & Retention Analytics           â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Data Storage Layers

| Layer | Technology | Purpose | Data Volume | Retention |
|-------|-----------|---------|-------------|-----------|
| **OLTP** | PostgreSQL 16+ | Transactional data | 10K users Ã— 50 actions/month = 500K rows/month | Indefinite |
| **Cache** | Redis 7.2+ | Real-time metrics, session data | 10K concurrent users Ã— 5KB = 50MB | 7 days (sessions), 1 hour (metrics) |
| **Analytics** | PostgreSQL Views | Aggregated analytics | Materialized views refresh every 15 min | 90 days (raw), 2 years (aggregated) |
| **Archive** | S3/Cold Storage | Historical data | After 2 years | Indefinite (compliance) |

---

## 2. Data Pipeline Design

### 2.1 Pipeline Architecture Pattern

**Pattern**: **ELT** (Extract-Load-Transform) for modern data stack
- **Why ELT over ETL**: Load raw data first, transform in PostgreSQL, preserve data lineage
- **Processing**: Batch (daily aggregations) + Real-time (event streams via triggers)

### 2.2 Core Data Pipelines

#### Pipeline 1: User Activity Tracking

**Purpose**: Track user actions for engagement analytics and point economy

```
User Action (API) â†’ PostgreSQL Insert â†’ Trigger â†’ Redis Counter Update
                                     â†“
                               transactions table
                                     â†“
                          Daily Aggregation Job
                                     â†“
                           user_activity_daily table
```

**Implementation**:
```sql
-- Real-time activity tracking table
CREATE TABLE user_activity_events (
    id BIGSERIAL PRIMARY KEY,
    user_id BIGINT NOT NULL REFERENCES users(id),
    event_type VARCHAR(50) NOT NULL,  -- 'post_created', 'comment_created', 'like_given', etc.
    event_data JSONB,  -- Flexible event metadata
    created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
);

-- Daily aggregation table (batch pipeline)
CREATE TABLE user_activity_daily (
    id BIGSERIAL PRIMARY KEY,
    user_id BIGINT NOT NULL REFERENCES users(id),
    activity_date DATE NOT NULL,

    -- Engagement metrics
    posts_created INT DEFAULT 0,
    comments_created INT DEFAULT 0,
    likes_given INT DEFAULT 0,
    likes_received INT DEFAULT 0,

    -- Point economy
    points_earned INT DEFAULT 0,
    points_spent INT DEFAULT 0,
    net_points INT DEFAULT 0,

    -- Time metrics
    session_count INT DEFAULT 0,
    total_session_duration INT DEFAULT 0,  -- seconds

    -- Timestamps
    created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,

    UNIQUE(user_id, activity_date)
);

-- Create index for fast lookups
CREATE INDEX idx_user_activity_daily_date ON user_activity_daily(activity_date DESC);
CREATE INDEX idx_user_activity_daily_user ON user_activity_daily(user_id, activity_date DESC);
```

**Batch Job** (runs daily at 2 AM UTC):
```python
# Daily aggregation ETL job
async def aggregate_daily_user_activity(date: datetime.date):
    """
    Aggregate user activity for a specific date.
    Run via Celery Beat scheduler.
    """
    query = """
    INSERT INTO user_activity_daily (
        user_id, activity_date,
        posts_created, comments_created, likes_given, likes_received,
        points_earned, points_spent, net_points
    )
    SELECT
        u.id AS user_id,
        :date AS activity_date,

        -- Count posts
        COUNT(DISTINCT p.id) AS posts_created,

        -- Count comments
        COUNT(DISTINCT c.id) AS comments_created,

        -- Count likes given
        COUNT(DISTINCT l.id) AS likes_given,

        -- Count likes received (on user's posts/comments)
        (
            SELECT COUNT(*) FROM likes
            WHERE (post_id IN (SELECT id FROM posts WHERE user_id = u.id)
                   OR comment_id IN (SELECT id FROM comments WHERE user_id = u.id))
              AND DATE(created_at) = :date
        ) AS likes_received,

        -- Points earned
        COALESCE(SUM(CASE WHEN t.amount > 0 THEN t.amount ELSE 0 END), 0) AS points_earned,

        -- Points spent
        COALESCE(ABS(SUM(CASE WHEN t.amount < 0 THEN t.amount ELSE 0 END)), 0) AS points_spent,

        -- Net points
        COALESCE(SUM(t.amount), 0) AS net_points

    FROM users u
    LEFT JOIN posts p ON p.user_id = u.id AND DATE(p.created_at) = :date
    LEFT JOIN comments c ON c.user_id = u.id AND DATE(c.created_at) = :date
    LEFT JOIN likes l ON l.user_id = u.id AND DATE(l.created_at) = :date
    LEFT JOIN transactions t ON t.user_id = u.id AND DATE(t.created_at) = :date
    WHERE u.is_active = TRUE
    GROUP BY u.id
    ON CONFLICT (user_id, activity_date)
    DO UPDATE SET
        posts_created = EXCLUDED.posts_created,
        comments_created = EXCLUDED.comments_created,
        likes_given = EXCLUDED.likes_given,
        likes_received = EXCLUDED.likes_received,
        points_earned = EXCLUDED.points_earned,
        points_spent = EXCLUDED.points_spent,
        net_points = EXCLUDED.net_points,
        updated_at = CURRENT_TIMESTAMP;
    """

    await db.execute(query, {"date": date})
    await db.commit()
```

---

#### Pipeline 2: Content Performance Analytics

**Purpose**: Track post and comment performance for trending algorithm and content insights

```sql
-- Content performance daily snapshots
CREATE TABLE content_performance_daily (
    id BIGSERIAL PRIMARY KEY,
    content_type VARCHAR(20) NOT NULL,  -- 'post' or 'comment'
    content_id BIGINT NOT NULL,
    snapshot_date DATE NOT NULL,

    -- Engagement metrics
    view_count INT DEFAULT 0,
    like_count INT DEFAULT 0,
    comment_count INT DEFAULT 0,
    share_count INT DEFAULT 0,

    -- Velocity metrics (change from previous day)
    views_delta INT DEFAULT 0,
    likes_delta INT DEFAULT 0,
    comments_delta INT DEFAULT 0,

    -- Calculated scores
    trending_score DECIMAL(10, 2) DEFAULT 0,  -- (likes*10 + comments*5) / hours_since_creation
    quality_score DECIMAL(10, 2) DEFAULT 0,   -- likes / views (engagement rate)

    -- Timestamps
    created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,

    UNIQUE(content_type, content_id, snapshot_date)
);

CREATE INDEX idx_content_perf_trending ON content_performance_daily(trending_score DESC, snapshot_date DESC);
CREATE INDEX idx_content_perf_date ON content_performance_daily(snapshot_date DESC);
```

**Batch Job** (runs hourly for trending posts):
```python
async def update_trending_scores():
    """
    Update trending scores for posts created in last 7 days.
    Run via Celery Beat every hour.
    """
    query = """
    UPDATE posts
    SET trending_score = (
        (like_count * 10 + comment_count * 5) /
        NULLIF(EXTRACT(EPOCH FROM (CURRENT_TIMESTAMP - created_at)) / 3600, 0)
    )
    WHERE created_at > CURRENT_TIMESTAMP - INTERVAL '7 days'
      AND is_deleted = FALSE;
    """
    await db.execute(query)
    await db.commit()
```

---

#### Pipeline 3: Point Economy Analytics

**Purpose**: Monitor point economy health, detect inflation/deflation, track reward distribution

```sql
-- Point economy daily metrics
CREATE TABLE point_economy_daily (
    id BIGSERIAL PRIMARY KEY,
    metric_date DATE NOT NULL UNIQUE,

    -- Supply metrics
    total_points_in_circulation BIGINT DEFAULT 0,  -- SUM(users.points)
    total_points_earned_today INT DEFAULT 0,
    total_points_spent_today INT DEFAULT 0,
    net_points_change INT DEFAULT 0,  -- earned - spent

    -- Transaction metrics
    registration_bonuses INT DEFAULT 0,  -- +100 per signup
    post_creation_costs INT DEFAULT 0,   -- -5 per post
    comment_creation_costs INT DEFAULT 0,  -- -2 per comment
    like_costs INT DEFAULT 0,  -- -1 per like
    like_rewards INT DEFAULT 0,  -- +3/+30/+350 rewards
    crypto_redemptions INT DEFAULT 0,  -- -10,000 per BNB claim

    -- User distribution
    users_with_points INT DEFAULT 0,
    median_user_balance INT DEFAULT 0,
    top_1_percent_balance BIGINT DEFAULT 0,  -- Concentration metric

    -- Health indicators
    inflation_rate DECIMAL(5, 2) DEFAULT 0,  -- % change from yesterday
    gini_coefficient DECIMAL(5, 4) DEFAULT 0,  -- Wealth inequality (0=equal, 1=unequal)

    -- Timestamps
    created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_point_economy_date ON point_economy_daily(metric_date DESC);
```

**Batch Job** (runs daily at 3 AM UTC):
```python
async def calculate_point_economy_metrics(date: datetime.date):
    """
    Calculate point economy health metrics.
    Monitors inflation, distribution, and potential exploits.
    """
    query = """
    INSERT INTO point_economy_daily (
        metric_date,
        total_points_in_circulation,
        total_points_earned_today,
        total_points_spent_today,
        net_points_change,
        users_with_points,
        median_user_balance
    )
    SELECT
        :date AS metric_date,

        -- Total points in circulation
        SUM(points) AS total_points_in_circulation,

        -- Points earned today
        COALESCE((
            SELECT SUM(amount) FROM transactions
            WHERE DATE(created_at) = :date AND amount > 0
        ), 0) AS total_points_earned_today,

        -- Points spent today
        COALESCE(ABS((
            SELECT SUM(amount) FROM transactions
            WHERE DATE(created_at) = :date AND amount < 0
        )), 0) AS total_points_spent_today,

        -- Net change
        COALESCE((
            SELECT SUM(amount) FROM transactions
            WHERE DATE(created_at) = :date
        ), 0) AS net_points_change,

        -- Users with non-zero balance
        COUNT(CASE WHEN points > 0 THEN 1 END) AS users_with_points,

        -- Median balance
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY points) AS median_user_balance

    FROM users
    WHERE is_active = TRUE
    ON CONFLICT (metric_date)
    DO UPDATE SET
        total_points_in_circulation = EXCLUDED.total_points_in_circulation,
        total_points_earned_today = EXCLUDED.total_points_earned_today,
        total_points_spent_today = EXCLUDED.total_points_spent_today,
        net_points_change = EXCLUDED.net_points_change,
        users_with_points = EXCLUDED.users_with_points,
        median_user_balance = EXCLUDED.median_user_balance;
    """

    await db.execute(query, {"date": date})

    # Calculate inflation rate (compare to yesterday)
    await calculate_inflation_rate(date)

    # Calculate Gini coefficient (wealth inequality)
    await calculate_gini_coefficient(date)

    await db.commit()
```

---

#### Pipeline 4: User Retention & Cohort Analysis

**Purpose**: Track user retention, cohort behavior, churn prediction

```sql
-- User cohorts (grouped by signup month)
CREATE TABLE user_cohorts (
    id BIGSERIAL PRIMARY KEY,
    cohort_month DATE NOT NULL,  -- First day of month
    cohort_size INT DEFAULT 0,   -- Users who signed up that month

    -- Retention metrics (% of cohort still active)
    retention_day_1 DECIMAL(5, 2) DEFAULT 0,
    retention_day_7 DECIMAL(5, 2) DEFAULT 0,
    retention_day_30 DECIMAL(5, 2) DEFAULT 0,
    retention_day_90 DECIMAL(5, 2) DEFAULT 0,

    -- Engagement metrics (average per active user)
    avg_posts_per_user DECIMAL(10, 2) DEFAULT 0,
    avg_comments_per_user DECIMAL(10, 2) DEFAULT 0,
    avg_points_earned DECIMAL(10, 2) DEFAULT 0,

    -- Timestamps
    created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,

    UNIQUE(cohort_month)
);

CREATE INDEX idx_user_cohorts_month ON user_cohorts(cohort_month DESC);
```

---

#### Pipeline 5: Moderation & Safety Analytics

**Purpose**: Track moderation effectiveness, detect abuse patterns, monitor content safety

```sql
-- Moderation metrics daily
CREATE TABLE moderation_metrics_daily (
    id BIGSERIAL PRIMARY KEY,
    metric_date DATE NOT NULL UNIQUE,

    -- Report metrics
    reports_submitted INT DEFAULT 0,
    reports_resolved INT DEFAULT 0,
    reports_pending INT DEFAULT 0,
    avg_resolution_time_hours DECIMAL(10, 2) DEFAULT 0,

    -- Action metrics
    posts_deleted INT DEFAULT 0,
    comments_deleted INT DEFAULT 0,
    users_banned_temp INT DEFAULT 0,
    users_banned_perm INT DEFAULT 0,

    -- Safety metrics
    spam_posts_detected INT DEFAULT 0,
    harassment_reports INT DEFAULT 0,
    hate_speech_reports INT DEFAULT 0,

    -- Moderator performance
    active_moderators INT DEFAULT 0,
    avg_reports_per_moderator DECIMAL(10, 2) DEFAULT 0,

    -- Timestamps
    created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_moderation_metrics_date ON moderation_metrics_daily(metric_date DESC);
```

---

## 3. Analytics & Reporting

### 3.1 Analytics Dashboard Specifications

#### Dashboard 1: Executive Overview

**Purpose**: High-level KPIs for product/business decisions

**Metrics**:
- **Users**: Total users, DAU, MAU, DAU/MAU ratio
- **Engagement**: Posts/day, Comments/day, Avg session duration
- **Economy**: Points in circulation, Crypto rewards distributed, Revenue (if applicable)
- **Growth**: New signups/day, Retention (D1, D7, D30), Churn rate

**Refresh**: Real-time (Redis counters) + Daily aggregations

**Visualization**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Executive Dashboard - Decentralized Forum           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  ðŸ“Š User Metrics                      ðŸ“ˆ Engagement          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Total Users  â”‚  â”‚     DAU      â”‚  â”‚  Posts/Day   â”‚      â”‚
â”‚  â”‚   10,245     â”‚  â”‚    1,523     â”‚  â”‚     342      â”‚      â”‚
â”‚  â”‚   +12% â†‘     â”‚  â”‚   +8% â†‘      â”‚  â”‚    +15% â†‘    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                               â”‚
â”‚  ðŸ’° Point Economy                     ðŸš€ Growth              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Circulation  â”‚  â”‚ BNB Rewards  â”‚  â”‚ Retention D7 â”‚      â”‚
â”‚  â”‚  5.2M pts    â”‚  â”‚   0.42 BNB   â”‚  â”‚     68%      â”‚      â”‚
â”‚  â”‚   +5% â†‘      â”‚  â”‚   +22% â†‘     â”‚  â”‚    +3% â†‘     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                               â”‚
â”‚  ðŸ“‰ Trend Charts (Last 30 Days)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ DAU Trend                                           â”‚     â”‚
â”‚  â”‚ 2000â”‚                                           â—   â”‚     â”‚
â”‚  â”‚ 1500â”‚                               â—       â—       â”‚     â”‚
â”‚  â”‚ 1000â”‚           â—       â—       â—                   â”‚     â”‚
â”‚  â”‚  500â”‚   â—   â—                                       â”‚     â”‚
â”‚  â”‚    0â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚     â”‚
â”‚  â”‚     Oct 1              Oct 15           Oct 30      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation**:
```python
from fastapi import APIRouter
from pydantic import BaseModel

router = APIRouter()

class ExecutiveDashboardMetrics(BaseModel):
    # User metrics
    total_users: int
    dau: int  # Daily Active Users
    mau: int  # Monthly Active Users
    dau_mau_ratio: float

    # Engagement metrics
    posts_per_day: int
    comments_per_day: int
    avg_session_duration_min: float

    # Economy metrics
    points_in_circulation: int
    bnb_rewards_distributed: float

    # Growth metrics
    new_signups_today: int
    retention_d1: float
    retention_d7: float
    retention_d30: float
    churn_rate: float

@router.get("/api/analytics/executive-dashboard", response_model=ExecutiveDashboardMetrics)
async def get_executive_dashboard():
    """
    Get executive dashboard metrics.
    Combines real-time Redis data with PostgreSQL aggregations.
    """
    # Get real-time metrics from Redis
    dau = await redis.get("metrics:dau:today")

    # Get aggregated metrics from PostgreSQL
    query = """
    SELECT
        (SELECT COUNT(*) FROM users WHERE is_active = TRUE) AS total_users,
        (SELECT COUNT(DISTINCT user_id) FROM user_activity_daily
         WHERE activity_date >= CURRENT_DATE - INTERVAL '30 days') AS mau,
        (SELECT COUNT(*) FROM posts WHERE DATE(created_at) = CURRENT_DATE) AS posts_today,
        (SELECT COUNT(*) FROM comments WHERE DATE(created_at) = CURRENT_DATE) AS comments_today,
        (SELECT SUM(points) FROM users WHERE is_active = TRUE) AS points_circulation,
        (SELECT SUM(amount_bnb) FROM blockchain_transactions
         WHERE status = 'confirmed') AS bnb_distributed
    """

    result = await db.fetch_one(query)

    return ExecutiveDashboardMetrics(
        total_users=result["total_users"],
        dau=int(dau) if dau else 0,
        mau=result["mau"],
        dau_mau_ratio=(int(dau) / result["mau"]) if result["mau"] > 0 else 0,
        posts_per_day=result["posts_today"],
        comments_per_day=result["comments_today"],
        # ... additional metrics
    )
```

---

#### Dashboard 2: User Engagement Analytics

**Metrics**:
- Active users by level (New User, Active User, Trusted User, Moderator, Admin)
- Engagement funnel (Signup â†’ First Post â†’ First Comment â†’ First Like â†’ Level Up)
- Session analytics (Duration, Pages per session, Bounce rate)
- Feature adoption (OAuth providers used, IPFS uploads, Wallet connections)

---

#### Dashboard 3: Point Economy Health

**Metrics**:
- Points in circulation (trend over time)
- Inflation/deflation rate
- Gini coefficient (wealth inequality)
- Top point earners and spenders
- Transaction type breakdown (earn vs spend)
- Crypto redemption rate

**Alert Thresholds**:
- âš ï¸ **High Inflation**: >10% daily increase in points supply
- âš ï¸ **Deflation**: Negative net points for 3+ consecutive days
- âš ï¸ **Inequality**: Gini coefficient >0.7 (top 1% hold 70%+ of points)

---

#### Dashboard 4: Content Performance

**Metrics**:
- Trending posts (hot algorithm score)
- Top channels by engagement
- Post quality distribution (likes/views ratio)
- Content velocity (posts/hour, comments/hour)
- Most active tags

---

### 3.2 Real-Time Metrics (Redis)

**Redis Key Structure**:
```
metrics:dau:today               â†’ Count of unique daily active users
metrics:posts:hourly:YYYYMMDDHH â†’ Posts created in last hour
metrics:likes:count             â†’ Total likes counter (fast increment)
metrics:user:{user_id}:online   â†’ User online status (TTL: 5 min)
leaderboard:points              â†’ Sorted set of user IDs by points
leaderboard:posts               â†’ Sorted set of user IDs by post count
```

**Implementation**:
```python
from redis import Redis

redis = Redis.from_url("redis://localhost:6379")

async def track_user_activity(user_id: int, action: str):
    """
    Track user activity in real-time using Redis.
    """
    # Increment DAU counter
    today = datetime.now().strftime("%Y%m%d")
    await redis.sadd(f"metrics:dau:{today}", user_id)
    await redis.expire(f"metrics:dau:{today}", 86400 * 7)  # 7-day TTL

    # Track user online status
    await redis.setex(f"metrics:user:{user_id}:online", 300, "1")  # 5-min TTL

    # Increment action counter
    hour = datetime.now().strftime("%Y%m%d%H")
    await redis.incr(f"metrics:{action}:hourly:{hour}")
    await redis.expire(f"metrics:{action}:hourly:{hour}", 86400)  # 24-hour TTL
```

---

## 4. Data Governance & Quality

### 4.1 Data Governance Framework

**Principles**:
1. **Privacy by Design**: GDPR compliance, data minimization
2. **Data Ownership**: Users own their data, can export/delete
3. **Transparency**: Clear data collection and usage policies
4. **Security**: Encryption at rest and in transit
5. **Auditability**: Complete data lineage, audit trails

### 4.2 GDPR Compliance

**User Data Rights**:
```sql
-- Data export: User can request all their data
CREATE OR REPLACE FUNCTION export_user_data(p_user_id BIGINT)
RETURNS JSONB AS $$
DECLARE
    user_data JSONB;
BEGIN
    SELECT jsonb_build_object(
        'user', (SELECT row_to_json(u.*) FROM users u WHERE id = p_user_id),
        'posts', (SELECT jsonb_agg(row_to_json(p.*)) FROM posts p WHERE user_id = p_user_id),
        'comments', (SELECT jsonb_agg(row_to_json(c.*)) FROM comments c WHERE user_id = p_user_id),
        'transactions', (SELECT jsonb_agg(row_to_json(t.*)) FROM transactions t WHERE user_id = p_user_id),
        'reports', (SELECT jsonb_agg(row_to_json(r.*)) FROM reports r WHERE reporter_id = p_user_id)
    ) INTO user_data;

    RETURN user_data;
END;
$$ LANGUAGE plpgsql;

-- Data deletion: User can request account deletion
CREATE OR REPLACE FUNCTION anonymize_user_data(p_user_id BIGINT)
RETURNS BOOLEAN AS $$
BEGIN
    -- Anonymize user profile (keep posts/comments for content integrity)
    UPDATE users SET
        email = 'deleted_' || id || '@example.com',
        username = 'deleted_user_' || id,
        password_hash = NULL,
        bio = NULL,
        avatar_url = NULL,
        bnb_wallet_address = NULL,
        is_active = FALSE,
        preferences = '{}'::jsonb
    WHERE id = p_user_id;

    -- Delete OAuth accounts
    DELETE FROM oauth_accounts WHERE user_id = p_user_id;

    -- Delete sessions
    DELETE FROM sessions WHERE user_id = p_user_id;

    -- Keep posts/comments attributed to "deleted_user_{id}" for content integrity

    RETURN TRUE;
END;
$$ LANGUAGE plpgsql;
```

### 4.3 Data Quality Monitoring

**Automated Quality Checks** (runs every hour):
```python
from enum import Enum
from typing import List

class DataQualityCheck(Enum):
    COMPLETENESS = "completeness"
    ACCURACY = "accuracy"
    CONSISTENCY = "consistency"
    TIMELINESS = "timeliness"
    VALIDITY = "validity"

async def run_data_quality_checks() -> List[dict]:
    """
    Run automated data quality checks.
    Returns list of failed checks with details.
    """
    failures = []

    # Check 1: User email uniqueness (consistency)
    duplicate_emails = await db.fetch_one(
        "SELECT COUNT(*) AS count FROM users GROUP BY email HAVING COUNT(*) > 1"
    )
    if duplicate_emails and duplicate_emails["count"] > 0:
        failures.append({
            "check": DataQualityCheck.CONSISTENCY,
            "rule": "user_email_uniqueness",
            "severity": "critical",
            "message": f"Found {duplicate_emails['count']} duplicate emails"
        })

    # Check 2: Point balance consistency (accuracy)
    inconsistent_balances = await db.fetch_one("""
        SELECT COUNT(*) AS count FROM users u
        WHERE u.points != (
            SELECT COALESCE(SUM(amount), 0)
            FROM transactions
            WHERE user_id = u.id
        )
    """)
    if inconsistent_balances and inconsistent_balances["count"] > 0:
        failures.append({
            "check": DataQualityCheck.ACCURACY,
            "rule": "point_balance_consistency",
            "severity": "critical",
            "message": f"Found {inconsistent_balances['count']} users with incorrect point balances"
        })

    # Check 3: Orphaned data (referential integrity)
    orphaned_posts = await db.fetch_one("""
        SELECT COUNT(*) AS count FROM posts
        WHERE user_id NOT IN (SELECT id FROM users)
    """)
    if orphaned_posts and orphaned_posts["count"] > 0:
        failures.append({
            "check": DataQualityCheck.CONSISTENCY,
            "rule": "referential_integrity_posts",
            "severity": "high",
            "message": f"Found {orphaned_posts['count']} orphaned posts"
        })

    # Check 4: Data freshness (timeliness)
    latest_activity = await db.fetch_one("""
        SELECT MAX(created_at) AS latest
        FROM user_activity_daily
    """)
    if latest_activity:
        hours_since_update = (datetime.now() - latest_activity["latest"]).total_seconds() / 3600
        if hours_since_update > 26:  # Should update daily
            failures.append({
                "check": DataQualityCheck.TIMELINESS,
                "rule": "daily_aggregation_freshness",
                "severity": "medium",
                "message": f"User activity aggregation is {hours_since_update:.1f} hours old (expected <26h)"
            })

    # Check 5: Data completeness (no NULL values in required fields)
    incomplete_users = await db.fetch_one("""
        SELECT COUNT(*) AS count FROM users
        WHERE email IS NULL OR username IS NULL
    """)
    if incomplete_users and incomplete_users["count"] > 0:
        failures.append({
            "check": DataQualityCheck.COMPLETENESS,
            "rule": "user_required_fields",
            "severity": "critical",
            "message": f"Found {incomplete_users['count']} users with missing required fields"
        })

    return failures
```

**Alert Configuration**:
- **Critical** failures â†’ Immediate Slack/email alert to on-call engineer
- **High** failures â†’ Alert within 1 hour
- **Medium** failures â†’ Daily summary report

---

## 5. Implementation Strategy

### 5.1 Phase 1: Foundation (Week 1-2)

**Tasks**:
1. âœ… Create analytics tables (user_activity_daily, content_performance_daily, etc.)
2. âœ… Set up Celery Beat scheduler for batch jobs
3. âœ… Implement basic Redis real-time counters
4. âœ… Create data quality monitoring framework

**Deliverables**:
- SQL migration scripts for analytics tables
- Celery tasks for daily aggregations
- Data quality check suite

---

### 5.2 Phase 2: Analytics Dashboards (Week 3-4)

**Tasks**:
1. Build Executive Dashboard API endpoints
2. Build User Engagement Dashboard
3. Build Point Economy Health Dashboard
4. Create basic data visualization (Chart.js or Plotly)

**Deliverables**:
- FastAPI endpoints for dashboard data
- Frontend dashboard pages (Jinja2 templates)

---

### 5.3 Phase 3: Advanced Analytics (Month 2)

**Tasks**:
1. Implement cohort analysis
2. Implement predictive analytics (churn prediction)
3. Set up automated reporting (weekly/monthly email reports)
4. Implement data export functionality (GDPR compliance)

---

### 5.4 Monitoring & Alerting

**Tools**:
- **Sentry**: Error tracking for data pipeline failures
- **Grafana + Prometheus**: Metrics visualization, data quality dashboards
- **Custom Alerts**: Slack/email notifications for critical data quality issues

**Key Metrics to Monitor**:
- Pipeline execution time (target: <5 min for daily aggregations)
- Data quality check pass rate (target: 100%)
- Cache hit rate (target: >80%)
- Query performance (target: P95 <100ms for dashboard queries)

---

## Conclusion

This data architecture provides a **scalable, privacy-compliant, and insight-driven** foundation for the Decentralized Autonomous Forum. Key strengths:

âœ… **Modern ELT Pattern**: Load raw data, transform in PostgreSQL, maintain data lineage
âœ… **Real-Time + Batch**: Redis for real-time metrics, PostgreSQL for analytical aggregations
âœ… **GDPR Compliant**: User data export, anonymization, privacy by design
âœ… **Data Quality**: Automated checks, monitoring, alerting
âœ… **Scalable**: PostgreSQL views, Redis caching, batch processing
âœ… **Cost-Effective**: No separate data warehouse needed (PostgreSQL handles both OLTP + OLAP)

**Next Steps**: Proceed to Develop Agent (`/develop`) for implementation of data pipelines and analytics APIs.

---

**Generated Files**:
- [x] data-architecture-20251021-200000.md (this document)
- [ ] ETL job implementations (next: Develop Agent)
- [ ] Analytics API endpoints (next: Develop Agent)
- [ ] Dashboard frontend (next: Develop Agent)
